{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed027501-1118-48c8-9653-fe1cd4540420",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Admin Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9637c-e2a4-4416-b432-6a395b59eb3f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Absolutely—here are practical, copy-pasteable patterns I see admins automate with Semantic Link Labs (+Sempy) in real Fabric environments. \n",
    "They’re written to be idempotent, service-principal friendly, and easy to drop into a Notebook task inside a Data Pipeline so you can schedule them.\n",
    "\n",
    "Setup (once per notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7983724-f1cf-4057-8c25-392908460af9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U semantic-link-labs sempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf21fe5-1d06-4701-8204-15ae610c0f1e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import time, json, pandas as pd\n",
    "import sempy_labs as labs\n",
    "from sempy_labs import admin, report, graph\n",
    "from sempy import fabric  # for workspace/items helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1349c-a3b2-4056-9635-9ae949edc694",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# (Optional) SPN auth via Key Vault; otherwise use user auth\n",
    "kv_uri = \"https://<your-keyvault>.vault.azure.net/\"\n",
    "with labs.service_principal_authentication(\n",
    "    key_vault_uri=kv_uri,\n",
    "    key_vault_tenant_id=\"<tenant-id>\",\n",
    "    key_vault_client_id=\"<client-id>\",\n",
    "    key_vault_client_secret=\"<client-secret>\"\n",
    "):\n",
    "    pass  # context active for all calls below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b9f61-ba8b-412e-b6bf-3d35281f5664",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 1) “Dataset swap night”: rebind every report from TEST → PROD\n",
    "\n",
    "# When you promote a Direct Lake/Import model, rebind all downstream reports in one go.\n",
    "\n",
    "old_ds = \"Enterprise Model (TEST)\"\n",
    "new_ds = \"Enterprise Model (PROD)\"\n",
    "\n",
    "# Rebind all reports that point to the old dataset, across all workspaces\n",
    "report.report_rebind_all(dataset=old_ds, new_dataset=new_ds, report_workspace=None)\n",
    "\n",
    "# Safety: verify nothing still points at the old dataset\n",
    "dangling = admin.list_reports(referenced_dataset=old_ds)\n",
    "assert len(dangling) == 0, f\"Dangling reports still bound to {old_ds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4c623-3afd-41a4-864b-5ff3fbada291",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 2) Auto-heal failed refreshes with backoff + owner notification\n",
    "\n",
    "# Useful when a transient failure is common (gateway hiccup, throttling).\n",
    "\n",
    "target_ds = \"Ops Finance Model\"\n",
    "ws = \"Ops BI\"\n",
    "\n",
    "hist = admin.list_refresh_history(dataset=target_ds, workspace=ws, top=1)\n",
    "if len(hist) and hist.iloc[0][\"Status\"] == \"Failed\":\n",
    "    for attempt in range(1, 4):  # 3 tries\n",
    "        try:\n",
    "            admin.trigger_dataset_refresh(dataset=target_ds, workspace=ws)\n",
    "            time.sleep(30 * attempt)  # backoff\n",
    "            latest = admin.list_refresh_history(dataset=target_ds, workspace=ws, top=1)\n",
    "            if latest.iloc[0][\"Status\"] == \"Completed\":\n",
    "                break\n",
    "        except Exception as e:\n",
    "            if attempt == 3:\n",
    "                owner = admin.get_dataset_owner(dataset=target_ds, workspace=ws)\n",
    "                graph.send_mail(\n",
    "                    user=\"me@contoso.com\",\n",
    "                    to_recipients=[owner],\n",
    "                    subject=f\"[Fabric] Refresh failed after retries: {target_ds}\",\n",
    "                    content=f\"<p>Last error: {e}</p>\"\n",
    "                )\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a976e80-4892-45fa-b6cf-1fb6942aec38",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 3) Capacity hot-spot watch: flag top refresh consumers & throttle with rules\n",
    "\n",
    "# Great to catch models hammering your capacity during business hours.\n",
    "\n",
    "cap_use = admin.get_refreshables()  # returns refreshables + recent durations/status\n",
    "offenders = (cap_use[cap_use[\"LastRefreshDuration\"] > 1800]  # >30 minutes\n",
    "             .sort_values(\"LastRefreshDuration\", ascending=False)\n",
    "             .head(10))\n",
    "\n",
    "# Email a daily digest\n",
    "if len(offenders):\n",
    "    html = \"<h3>Top Refresh Consumers (last 24h)</h3>\" + offenders.to_html(index=False)\n",
    "    graph.send_mail(\n",
    "        user=\"me@contoso.com\",\n",
    "        to_recipients=[\"bi-admins@contoso.com\"],\n",
    "        subject=\"[Fabric] Capacity hot-spot watch\",\n",
    "        content=html\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb639e61-8742-44e7-9bc5-613003ac151e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 4) Drift detection: tenant & delegated settings baseline compare\n",
    "\n",
    "# Catch unapproved changes to admin/tenant settings.\n",
    "\n",
    "# Load yesterday's baseline from Lakehouse/Files (parquet/json), compare to current\n",
    "current = admin.list_tenant_settings()\n",
    "yesterday = pd.read_parquet(\"Files/fabric_admin/tenant_settings/prev_snapshot.parquet\")\n",
    "\n",
    "diff = current.merge(yesterday, on=\"SettingName\", how=\"outer\", suffixes=(\"_now\",\"_prev\"), indicator=True)\n",
    "changed = diff.query(\"_merge!='both' or Value_now!=Value_prev'\")\n",
    "\n",
    "if len(changed):\n",
    "    changed.to_parquet(\"Files/fabric_admin/tenant_settings/diff.parquet\", index=False)\n",
    "    graph.send_mail(\n",
    "        user=\"me@contoso.com\",\n",
    "        to_recipients=[\"fabric-admins@contoso.com\"],\n",
    "        subject=\"[Fabric] Tenant settings drift detected\",\n",
    "        content=\"<p>See attached diff.</p>\",\n",
    "    )\n",
    "current.to_parquet(\"Files/fabric_admin/tenant_settings/prev_snapshot.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18449a6c-9bae-4ff8-8d51-a1db1631d876",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 5) “Publish to web” & org-wide links sweeper (weekly)\n",
    "\n",
    "# Reduce exposure risk.\n",
    "\n",
    "wide = admin.list_widely_shared_artifacts()            # org-wide links + publish-to-web\n",
    "risky = wide.query(\"IsPublishedToWeb == True or IsOrgWide == True\")\n",
    "if len(risky):\n",
    "    graph.send_mail(\n",
    "        user=\"me@contoso.com\",\n",
    "        to_recipients=[\"security@contoso.com\",\"bi-admins@contoso.com\"],\n",
    "        subject=\"[Fabric] Widely shared artifacts detected\",\n",
    "        content=risky.to_html(index=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a2045e-7767-401d-a2cc-1fd911e3399f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 6) “Leaver” automation: remove user access across workspaces & models\n",
    "\n",
    "# Hook this up to your HR offboarding feed.\n",
    "\n",
    "ex_user = \"former.user@contoso.com\"\n",
    "# Remove from every workspace they belong to\n",
    "ent = admin.list_access_entities(user_email_address=ex_user)\n",
    "for _, r in ent.iterrows():\n",
    "    try:\n",
    "        admin.remove_workspace_user(workspace=r[\"WorkspaceName\"], principal=ex_user)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Remove from all dataset roles\n",
    "roles = admin.list_dataset_roles(user_email_address=ex_user)\n",
    "for _, r in roles.iterrows():\n",
    "    admin.remove_dataset_role_member(dataset=r[\"DatasetName\"], workspace=r[\"WorkspaceName\"], principal=ex_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd57a5-2d8b-4dd3-8e83-896402d40564",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 7) Gateway hygiene: verify bindings + orphaned data sources\n",
    "\n",
    "# Catch datasets that should use a gateway but aren’t bound, and fix them.\n",
    "\n",
    "targets = admin.list_datasets(needs_gateway=True)  # filter helper\n",
    "gws = labs.list_gateways()\n",
    "default_gw = gws.iloc[0]['Id']  # pick your standard cluster\n",
    "\n",
    "for _, row in targets.iterrows():\n",
    "    try:\n",
    "        labs.bind_dataset_to_gateway(\n",
    "            dataset=row[\"Name\"], workspace=row[\"WorkspaceName\"], gateway=default_gw\n",
    "        )\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c805ab-9fb4-410b-8f2b-32ab34fba80a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 8) Naming policy enforcement (workspaces/items)\n",
    "\n",
    "# Keep your catalog clean (domain-prefix, owner code, environment suffix).\n",
    "\n",
    "policy = {\"Workspace\": r\"^(WM|FICM|ECM)-[A-Za-z0-9\\-]+-(DEV|TEST|PROD)$\"}\n",
    "\n",
    "items = fabric.list_items()  # across all workspaces you can see\n",
    "bad_ws = items.drop_duplicates(\"WorkspaceName\").query(\"~WorkspaceName.str.match(@policy['Workspace'])\")\n",
    "if len(bad_ws):\n",
    "    # Send owners a gentle “please rename” notice (include examples)\n",
    "    owners = bad_ws.groupby(\"WorkspaceName\")[\"WorkspaceOwner\"].first().tolist()\n",
    "    graph.send_mail(\n",
    "        user=\"me@contoso.com\",\n",
    "        to_recipients=owners,\n",
    "        subject=\"[Fabric] Workspace naming policy reminder\",\n",
    "        content=bad_ws[[\"WorkspaceName\"]].to_html(index=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d022e2b-c94a-4a35-8bd2-c49662e0aac0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 9) Bulk assign workspaces to capacity (after tenant re-org)\n",
    "target_capacity = \"Premium-Capacity-A\"\n",
    "to_move = [\"WM-Research-Prod\",\"WM-OCIO-Prod\"]\n",
    "\n",
    "for ws in to_move:\n",
    "    admin.assign_workspace_to_capacity(workspace=ws, capacity_name=target_capacity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8e3cd-73e7-4b6c-a4a0-17751ff51304",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 10) Direct Lake migration assist: update connections then rebind\n",
    "# 1) Convert/point dataset to Direct Lake source (tables=None -> all)\n",
    "from sempy_labs import directlake\n",
    "directlake.update_direct_lake_model_connection(dataset=\"Trading DL\", workspace=\"Ops BI\")\n",
    "\n",
    "# 2) Rebind dependent reports (see #1)\n",
    "report.report_rebind_all(dataset=\"Trading DL (Old)\", new_dataset=\"Trading DL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6707a-6a9a-472a-b0d5-21a10a3c5784",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 11) Programmatic refresh schedules (standardize cadence)\n",
    "targets = [\n",
    "  {\"dataset\":\"WM Sales Model\",\"workspace\":\"WM Analytics\",\"days\":[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\"],\"times\":[\"06:30\",\"12:00\"]},\n",
    "  {\"dataset\":\"ECM KPIs\",\"workspace\":\"ECM Analytics\",\"days\":[\"Monday\"],\"times\":[\"07:00\"]}\n",
    "]\n",
    "\n",
    "for t in targets:\n",
    "    admin.update_refresh_schedule(\n",
    "        dataset=t[\"dataset\"], workspace=t[\"workspace\"],\n",
    "        days=t[\"days\"], times=t[\"times\"], time_zone=\"Central Standard Time\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3510e27-8b5e-4e91-8d52-3499c2e84f5b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 12) RLS audit: enumerate roles, members, and orphaned emails\n",
    "rls = admin.list_dataset_roles()\n",
    "# Find members not in your AAD anymore (join to your HR export / Entra dump)\n",
    "aad = pd.read_csv(\"Files/security/entra_users.csv\")  # columns: UserPrincipalName\n",
    "rls[\"IsActiveAAD\"] = rls[\"MemberEmail\"].isin(aad[\"UserPrincipalName\"])\n",
    "stale = rls.query(\"IsActiveAAD == False\")\n",
    "stale.to_parquet(\"Tables/rls_orphans.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2614db-3f39-485a-a75e-b999e7e36e05",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 13) Git hygiene: report on connected vs. unconnected workspaces\n",
    "git = admin.list_git_connections()              # connected\n",
    "all_ws = admin.list_workspaces()                # all\n",
    "joined = all_ws.merge(git[[\"WorkspaceName\",\"RepositoryUrl\"]], how=\"left\", on=\"WorkspaceName\")\n",
    "unconnected = joined[joined[\"RepositoryUrl\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e2618-c70c-499e-b474-a6dd2dbca31f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 14) “No owner” & “no contact” find-and-fix\n",
    "ws = admin.list_workspaces()\n",
    "no_owner = ws[ws[\"WorkspaceOwner\"].isna() | (ws[\"WorkspaceOwner\"]==\"\")]\n",
    "for _, r in no_owner.iterrows():\n",
    "    # Add a default owner group to ensure accountability\n",
    "    admin.add_workspace_user(workspace=r[\"WorkspaceName\"], principal=\"bi-admins@contoso.com\", role=\"Admin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6f3bb-6592-4c45-a1d2-38c7e1e03808",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 15) License/cost hygiene: unused artifacts & “zombie” models cleanup\n",
    "unused = admin.list_unused_artifacts(days=45)  # your threshold\n",
    "zombies = unused.query(\"ArtifactType=='Dataset' and LastRefreshDate.isna()\")\n",
    "# Notify owners; after grace period, delete or archive\n",
    "for _, r in zombies.iterrows():\n",
    "    graph.send_mail(\n",
    "        user=\"me@contoso.com\",\n",
    "        to_recipients=[r[\"OwnerEmail\"]],\n",
    "        subject=f\"[Fabric] {r['Name']} scheduled for archive\",\n",
    "        content=f\"<p>No refresh & no usage in 45d. Reply if you need it retained.</p>\"\n",
    "    )\n",
    "    # admin.delete_dataset(dataset=r['Name'], workspace=r['WorkspaceName'])  # if you truly want to automate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0686167-c512-483a-8429-64672f809e9f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "How I’d operationalize this in Fabric\n",
    "\n",
    "Put each block in its own notebook (or parameterize one “admin runbook” notebook).\n",
    "\n",
    "Orchestrate with a Data Pipeline (sequential or parallel activities).\n",
    "\n",
    "Land outputs (diffs, inventories, audits) to Lakehouse tables (Delta).\n",
    "\n",
    "Build a tiny Admin Dashboard on top (DirectLake) for proof and traceability.\n",
    "\n",
    "Gate any destructive actions (delete, rebind) with a feature flag variable so you can run in “report-only” first.\n",
    "\n",
    "If you tell me your workspace naming conventions, capacity names, and which tasks you want first, \n",
    "I’ll package these into a ready-to-run pipeline with parameters (and add broker-specific checks like BETA Host / Envestnet tags)."
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
